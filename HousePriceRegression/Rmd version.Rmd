---
title: "Project 1"
author: "Carla Brookey, Fred Hintz, Thanh Tran, Kassidie Stokes"
date: "March 21, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo = FALSE, message=FALSE, results='hide', warning=FALSE}
require(dplyr)
library(stats)
library(Metrics)
library(glmnet)
library(ggplot2)
library(caret)
library(xgboost)
library(randomForest)
library(car)
library(reshape2)
```

First we read in the data to R. 
```{r}
HP.train <- read.csv('train.csv', sep = ',', header = TRUE)
HP.test <- read.csv('test.csv', sep = ',', header = TRUE)
HP.train$Id <- NULL
HP.test.Id <- HP.test$Id
HP.test$Id <- NULL
HP.test$SalePrice <- replicate(nrow(HP.test), 0)
ntrain <- nrow(HP.train)
ntest <- nrow(HP.test)
HP.all <- rbind(HP.train, HP.test)
```

Then we created functions to check for and deal with missing values.
We chose to encode categorical missing values as a separate "missing" category and to code missing continuous values as the mean of the variable. We determined that mean imputation prior to doing the variable selection technique was preferable because we were not certain that the variables we coded provided much predictive information. If they turned out to be important after performing variable selection, we decided that we would deal with the missing values more appropriately. 
```{r}
check_missing_values <- function(dataSet)
{
  for (i in seq(1,ncol(dataSet))) {
    num_NA = sum(is.na(dataSet[, i]))
    is_numeric = is.numeric(dataSet[,i]) 
    if (num_NA > 0) {
      print (paste0('column ', names(dataSet)[i],' has #missing values: ', num_NA, ' is numeric: ',is_numeric ))
    }
  }
}
check_categorical_variables <- function(dataSet)
{
  print('************************************************')
  print('Listing all variables with categorical values:')
  count <- 0
  for (i in 1:ncol(dataSet)) {
    if (!is.numeric(dataSet[,i])) {
      count <- count + 1
      factor_names <- paste(levels(dataSet[,i]), sep=" ")
      print (paste0(names(dataSet)[i], ' is categorical variable with number of factors: '))
      print (factor_names)
    }
  }
  print (paste0('Total categorical variables:' , count))
  print('************************************************')
}

deal_missing_values <- function(dataSet, ntrain = 1460, type = 1)
{
  # type is the replace method for numeric values. 
  #if type = 1 --> using mean to replace.
  #if type = 2 --> using median to replace.
  dataSet[] <- lapply(dataSet, function(x){
    # check if variables have a factor:
    if(!is.factor(x)) {
      #replace NA by mean
      if (type == 1) x[is.na(x)] <- mean(x[1:1460], na.rm = TRUE) 
      else if (type == 2) if (type == 1) x[is.na(x)] <- median(x[1:1460], na.rm = TRUE) 
    }
    else {
      # otherwise include NAs into factor levels and change factor levels:
      x <- factor(x, exclude=NULL)
      levels(x)[is.na(levels(x))] <- "Missing"
    }
    return(x)
  })
  
  return(dataSet)
}

#
# encoding categorical variables. If a variable has 3 levels --> we need 2 bit to encode
#
```

The following function was used to expand the categorical variables into dummy variables for the regression. 
```{r}
encode_categorical_variables <- function(dataSet, ntrain=1460)
{
  dataSetTemp <- dataSet
  for (i in 1:ncol(dataSet)){
    if (is.factor(dataSet[,i])) {
      col_name = names(dataSet)[i]
      t <- model.matrix(~ factor(dataSet[,i]) - 1)
      dataSetTemp <- cbind(t[,2:ncol(t)], dataSetTemp)
      new_col_names <- paste(col_name, "_bit", seq(1, ncol(t) - 1))
      names(dataSetTemp)[1:ncol(t) - 1] <- new_col_names
    }
  }
  removed <- c()
  for (i in 1:ncol(dataSetTemp)) {
    if (is.factor(dataSetTemp[,i])) removed <- c(removed, i)
  }
  dataSetTemp <- dataSetTemp[, -removed]
  return(dataSetTemp)
}
```

This function was one approach we used to identify outliers. It deletes any observation with an observation on any variable more than 1.5*IQR beyond the 25th and 75th percentile. We determined it wasn't useful for us because it deleted the entire dataset.
```{r}
deal_outliers <- function(dataSet)
{
  for (i in 1:ncol(dataSet)) {
    if (is.numeric(dataSet[,i])) {
      lower_bound = as.integer(quantile(dataSet[,i], .25) - 1.5*IQR(dataSet[,i]))
      upper_bound = as.integer(quantile(dataSet[,i], .75) + 1.5*IQR(dataSet[,i]))
      dataSet <- dataSet[dataSet[,i] >  lower_bound & dataSet[,i] < upper_bound, ]
    }
  }
  return(dataSet)
}
```

This function uses the elastic net technique to determine the most important variables for predicting the sale price. 
```{r}
do_variable_selection <- function(dataSet)
{
  trainData <- dataSet[1:ntrain,]
  logSalePrice <- log(trainData$SalePrice)
  trainData <- cbind(trainData, logSalePrice)
  trainData$SalePrice <- NULL
  trainData.cv.lasso = cv.glmnet(x = as.matrix(trainData[,1:(ncol(trainData) - 1)]), 
                             y=as.matrix(trainData[,ncol(trainData)]), alpha = 1)
  names(trainData.cv.lasso$lambda.1se)="Lambda 1 SE:"
  trainData.cv.lasso$lambda.1se
  trainData.lasso = glmnet(x = as.matrix(trainData[,1:(ncol(trainData) - 1)]), 
                           y=as.matrix(trainData[,ncol(trainData)]), alpha = 1)
  co <- coef(trainData.lasso, s = trainData.cv.lasso$lambda.1se)
  co <- co[2:nrow(co),] #remove the coefficient of intercept, which is the first row
  trainData <- trainData[, co > 0]
  vars <- names(trainData)
  print('Important variables:')
  print(vars)
  vars <- c(vars, 'SalePrice')
  dataSet <- subset(dataSet, select = vars)
  return(dataSet)
}
```


```{r,include=FALSE}
build_model <- function(train, type = 2, transformed = FALSE)
{
  
  if (type == 1) {
    #multiple linear regression model
    fit <- lm(SalePrice ~., train)  
    fit$rsquared <- summary(fit)$r.squared
    fit$adj.rsquared <- summary(fit)$adj.r.squared
    fit$predicted <- fit$fitted.values
      
  }
  if (type == 2) {
    #add elastic net regularization to reduce model overfitting
    # fit = cv.glmnet(x = as.matrix(train[,1:(ncol(train) - 1)]), y=as.matrix(train[,ncol(train)]), type.measure = "mse")
    # plot(fit)
    set.seed(1024)
    fit = glmnet(x = as.matrix(train[,1:(ncol(train) - 1)]), y=as.matrix(train[,ncol(train)]), family="gaussian", alpha = 0.1, lambda = 0.1)
    # plot(fit, xvar = "dev", label = TRUE)
    # plot(fit)
    # print(fit$lambda.min)
    # print(fit$lambda.lse)
    fit$predicted <- predict(fit, newx = as.matrix(train[,1:(ncol(train) - 1)]),  type="response", family="gaussian", standardize=TRUE)
  }
  if (type == 3) {
    # random forest in here:
    fit <- randomForest(x = as.matrix(train[,1:(ncol(train) - 1)]), y = as.matrix(train[,ncol(train)]), 
                        ntree = 200, mtry = 100)
    plot(fit)
    # fit$predicted <- as.vector(fit$predicted)
  }
  if (type == 4) {
    #xgboost here
    param <- list(max.depth=2,eta=0.1,nthread = 8, silent=1,objective='reg:linear')
    #param <- list(max.depth = 4,verbose = FALSE, eta = 0.1, nthread = 8, objective = "binary:logistic",task="pred")
    dataAll <- xgb.DMatrix(data=data.matrix(train[,1:(ncol(train)-1)]), 
                           label=data.matrix(train[,ncol(train)]))
    
    res <- xgb.cv(params = param, data = dataAll,  nrounds = 1000, nfold = 10, 
                  prediction = TRUE, eval_metric="rmse", verbose = TRUE)
    max_round = which.min(res$dt[, test.rmse.mean])
    rmse <- res$dt[, test.rmse.mean][max_round]
    
    fit <- xgboost(params = param, data=dataAll, nrounds = max_round)
  }
  if (type != 4) {
    fit$actual <- train$SalePrice
    if (transformed) {
      #reconvert
      fit$predicted <- reverse_transformed_response(fit$predicted)
      fit$actual <- reverse_transformed_response(fit$actual)
    }  
    
    fit$rmse <- rmse(actual = fit$actual, predicted  = fit$predicted)
    fit$mse <- mse(actual = fit$actual, predicted  = fit$predicted)
    print(paste0('RMSE of training :',fit$rmse))
  }
  return(fit)
}
do_transform <- function(dataSet)
{
  for (i in 1:(ncol(dataSet) - 1) ) {
    if (min(dataSet[,i]) == 0 && max(dataSet[,i]) == 1 ) {
      #categorical values --> do nothing
    } else{
      dataSet[,i] <- log(dataSet[,i] + 1)
    }
  }
  dataSet[,ncol(dataSet)] <- log(dataSet[,ncol(dataSet)] + 1)
  return(dataSet)
}
reverse_transformed_response <- function(response)
{
  return(exp(response) - 1)
}
evaluate_model <- function(model, test)
{
  eval.predicted <- predict(model, data=test)
  eval.actual <- test$SalePrice
  eval.RMSE <- rmse(actual = eval.actual, predicted = eval.predicted)
  
}

draw_boxplot <- function(data)
{
  vars <- c()
  for(i in 1:ncol(data)) {
    if (is.numeric(data[,i])) vars <- c(vars, names(data)[i])
  }
  group_size = 2
  steps = seq(1,length(vars), group_size)
  if (steps[length(steps)] < length(vars)) steps <- c(steps, length(vars) + 1)
  dev.off()
  #grouping 4 in a plot
  for(i in 1:(length(steps) - 1)) {
    from = steps[i]
    to = steps[i + 1]
    labels <- vars[from : (to - 1)]
    data_reshape <- melt(data[,labels], measure.vars = labels)
    p <- ggplot(data_reshape) + geom_boxplot(aes(x=variable, y=value, color=variable))
    print(p)  
  }
  
}
```

<!-- #missing values: categorical + numeric values. Numeric values can be replaced by mean, median for ease. -->
<!-- #outliers: do later. -->
<!-- #do regression: variable selection + linear regression + xgboost + random forest -->
<!-- # check the residuals -->
<!-- # Mine: regression part + transformation  -->

## 1. Dealing with missing values for training and testing data ##

First we checked to see which variables contained missing values. 
```{r}
check_missing_values(HP.all)
HP.all <- deal_missing_values(HP.all, ntrain)
print ('check missing values of the dataframe after replacing missing values. Should print blank')
check_missing_values(HP.all)
```

#2. Linear regression model with untransformed sale price data and residual analysis #

```{r}
HP.train<-deal_missing_values(HP.train)


HP.train.lr_model <- lm(SalePrice~., HP.train)
```

# *Residual Plots*

#Residual vs Fitted Plot
```{r}
plot(HP.train.lr_model, which=1)
```

There's a pretty clear expansion in variance once the 
 sale price goes above 3e+05. This Makes sense, since there 
are fewer homes at that price range.
The base r plots point out that the 3 biggest outliers are obs. # 826, 1325, 524

```{r}
```
#QQ plot
```{r}
plot(HP.train.lr_model, which=2)
```


The QQ plot looks pretty bad at the tails. 
Obs. 826 and 524 are again noted as outliers

#fitted values by square root of standardized residuals
```{r}
plot(HP.train.lr_model, which=3)
```


From examining this plot, the error clearly increases at higher ends of sale price

#cook's distance
```{r}
plot(HP.train.lr_model, which=4)
```

3 outliers in terms of cook's distance: 524, 1171, and 1424

#Leverage vs. standardized residuals
```{r}
plot(HP.train.lr_model, which=5)
```


observations with high leverage and high standardized residuals 
 are the same that have high cook's d

We have five clear outliers that need looking at. Log transform should help.
These are: 524,826,1171,1325, 1424



Based on the residual plots, we determined that a log transform of the saleprice variable was appropriate. 

#3. Linear regression model with log transform of sale price data and residual analysis 
```{r}
HP.trainlog<-HP.train %>% mutate(logSalePrice=log(SalePrice)) %>% select (-SalePrice)

HP.train_logtrans_mod<-lm(logSalePrice ~., HP.trainlog)
```

We looked at the same Plots again

The outliers from the untransformed model were 524,826,1171,1325, 1424

#Residuals vs fitted plot
```{r}
plot(HP.train_logtrans_mod, which=1, id.n=5)
```

 Residuals over .5 also now include observations 633 and 463


```{r}
plot(HP.train_logtrans_mod, which=2, id.n=6)
```

Observation 89 shows up as overestimating the price based on the trend line of the quantiles. The QQ plot for this model does not look much better than previous
```{r}
plot(HP.train_logtrans_mod, which=3)
```
The scale-location plot looks far better for this model, although error is slightly higher at the lower end now. The same outliers are identified
```{r}
plot(HP.train_logtrans_mod, which=4)
```

Obs. 89 is now identified as an influential observation by cook's distance.

```{r}
plot(HP.train_logtrans_mod, which=5)
```

Only a few observations with high leverage and high standardized residuals, which we already knew about.
```{r}
cat("Log trans r squared is", summary(HP.train_logtrans_mod)$adj.r.squared)
cat("\n untransformed r squared is", summary(HP.train.lr_model)$adj.r.squared)

cat(" \n Log trans F is", summary(HP.train_logtrans_mod)$fstatistic[[1]])
cat("\n untransformed F is", summary(HP.train.lr_model)$fstatistic[[1]])


```


Comparing with the untransformed model, we get a higher adjusted R-squared and a higher F statistic when predicting the log transform of sale price. 

# 2. Encoding categorical variables 

Before conducting the lasso variable selection method , we constructed dummy variables for the categorical variables and transformed some quantitative variables to make them more interpretable. 
```{r}
#check_categorical_variables(HP.train)

# HP.train <- encode_categorical_variables(HP.train)
HP.all <- encode_categorical_variables(HP.all)
print ('check categorical values of the dataframe after encoding. Should print blank')
check_categorical_variables(HP.all)

#reconstruct some variables:
HP.all <- mutate(HP.all,
                 house_age = YrSold + MoSold/12 - YearBuilt,
                 remodel_house_age = abs(YearRemodAdd - YearBuilt),
                 gara_age = abs(GarageYrBlt - YearBuilt),
                 sold_year_to_now = 2017 - YrSold - MoSold/12,
                 built_year_to_now = 2017 - YearBuilt
                 
)

SalePrice <- HP.all$SalePrice
HP.all$SalePrice <- NULL
HP.all$SalePrice <- SalePrice
HP.all$GarageYrBlt <- NULL
HP.all$YearBuilt <- NULL
HP.all$YearRemodAdd <- NULL
HP.all$YrSold <- NULL
HP.all$MoSold <- NULL
```

```{r, include=FALSE}
do_transformation <- TRUE
if (do_transformation) {
  HP.all <- do_transform(HP.all)
}

```

Here we perform the variable selection method. The variable selection function also creates a subset of the chosen variables to use in the model. 
```{r}
HP.all <- do_variable_selection(HP.all)
HP.train = HP.all[1:ntrain,]
HP.test = HP.all[(ntrain+1) : nrow(HP.all),]
HP.test$SalePrice <- NULL
```

```{r,include=FALSE}
for (i in 1:ncol(HP.train)) {
  if (sum(is.nan(HP.train[,i])) > 0 || sum(is.infinite(HP.train[,i])) > 0 ) {
    print (i)
  }
}
for (i in 1:ncol(HP.test)) {
  if (sum(is.nan(HP.test[,i])) > 0 || sum(is.infinite(HP.test[,i])) > 0 ) {
    print (i)
  }
}



print ('Building multiple linear regression model')
```


We then performed a linear regression with the lassoed variables on the log of the sale price. and looked at the residual plots. 

```{r}


#*******************************************************************************
###Multiple linear regression model with lassoed variables
#Make a new dataset with log of sale price
HP.train<-HP.all[1:ntrain,]

HP.trainlog_sub<-HP.train %>% mutate(logSalePrice=log(SalePrice)) %>% select(-SalePrice)

#Run a linear model on the log of sale price
HP_subset_linmod<-lm(logSalePrice ~ ., data=HP.trainlog_sub)

#View summary of model, R-Squared is still good
summary(HP_subset_linmod)
```

#Examining outliers from lassoed variable regression 
```{r}
plot(HP_subset_linmod,which=1)

```
 
 A few houses are severely overestimated. Why is this?
Check out cases 633, 524, and 1299, increasing in order of severity
```{r}
plot(HP_subset_linmod, which=2)
```

The same three outliers show up in qq plot, unsurprisingly

```{r}
plot(HP_subset_linmod, which=4)
```

Note that from this plot, 31 may also be an outlier, but if it is overestimated, we may not need to worry about it

```{r}
plot(HP_subset_linmod, which=5)
```

It's clear from these plots a few highly influential points are driving our regression

Look at these three points in detail

```{r}
HP.train[c(524,633,1299),]
```


Let's see what happens when we remove the three outliers that are clearly out of whack with our other points

```{r}
no_outliers<-c(1:523,525:1298,1300:1460)

HP_subset_linmod_noout<-lm(logSalePrice ~ ., 
                           data=HP.trainlog_sub,
                           subset=no_outliers)

summary(HP_subset_linmod_noout)

plot(HP_subset_linmod_noout,which=1)
```

When we remove those 3 points, the residuals look a lot
 better, our adjusted R-squared goes up, and our
 betas change a good deal, which means we may avoid fitting some noise by removing the poorly predicted points. 


We decided to use the model on the subset of the sample without those 3 outliers.

```{r}

#write to file
HP.test$SalePrice <- predict(HP_subset_linmod_noout, HP.test)
submission <- data.frame(Id <- HP.test.Id, SalePrice <- HP.test$SalePrice)
names(submission) <- c('Id', 'SalePrice')
write.csv(file = 'submission.csv', x = submission, row.names = FALSE)

```




