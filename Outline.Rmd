---
title: "Housing Prices Pt 2"
author: "Carla M Brookey, Fred Hintz, Kassidie Stokes, Thanh Tran"
date: "April 11, 2017"
output: pdf_document
---

```{r setup, include=FALSE, cache = TRUE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(56)
```

```{r 10foldFunction}
tenFoldCrossVal = function(formula, n = 10, data, type, ...){
  #n is the number of desired folds
  #data is the desired dataset
  #type is the type of analysis
  #formula is the formula for analysis
  
  folds = rep(c(1:n), length = nrow(data))
  folds = sample(folds)
  
  resultVector = rep(0, length(nrow(data)))
  
  for (i in 1:n){
    train = data[folds!=i,]
    test = data[folds==i,]
    fit = switch(type,
                 rpart = rpart(formula, data = train, ...),
                 randomForest = randomForest(formula, data = train, ...),
                 lda = lda(formula, data = train, ...),
                 qda = qda(formula, data = train, ...),
                 knn3 = knn3(formula, data = train, ...),
                 glm = glm(formula, data = train, ...),
                 ada = ada(formula, data = train, ...),
                 svm = svm(formula, data = train, ...),
                 gbm = gbm(formula, data = train, ...))
    
    resultVector[folds == i] = switch(type,
                                      rpart = predict(fit, newdata = test, ...),
                                      randomForest = predict(fit, newdata = test, ...),
                                      lda = predict(fit, data = test, ...)$class,
                                      qda = predict(fit, data = test, ...)$class,
                                      knn3 = predict(fit, data = test, ...),
                                      glm = predict(fit, data = test, ...),
                                      ada = predict(fit, data = test, ...),
                                      svm = predict(fit, data = test, ...),
                                      gbm = predict(fit, data = test, ...))
  }
  return(resultVector)
}
```

## R Markdown

Run with log(SalePrice).

```{r load, cache = TRUE, warning = FALSE}
#Load data
train <- read.csv('train.csv', sep = ',', header = TRUE)
test <- read.csv('test.csv', sep = ',', header = TRUE)

#Load Packages
library(rpart)
library(randomForest)
library(gbm)
library(caret)
library(e1071)
library(dplyr)
library(sparklyr)
library(neuralnet)
library(glmnet)

```

```{r rpart, cache = TRUE}

#Carla's doing random forest + gbm and uploading the 10-fold cv code

#than's doing xgboost 
#kassidie's doing svm
#Fred doing neural nets 


#Create a validation set
#' Splits data.frame into arbitrary number of groups
#' 
#' @param dat The data.frame to split into groups
#' @param props Numeric vector. What proportion of the data should
#'              go in each group?
#' @param which.adjust Numeric. Which group size should we 'fudge' to
#'              make sure that we sample enough (or not too much)
split_data <- function(dat, props = c(.8, .15, .05), which.adjust = 1){

    # Make sure proportions are positive
    # and the adjustment group isn't larger than the number
    # of groups specified
    stopifnot(all(props >= 0), which.adjust <= length(props))

    # could check to see if the sum is 1
    # but this is easier
    props <- props/sum(props)
    n <- nrow(dat)
    # How large should each group be?
    ns <- round(n * props)
    # The previous step might give something that
    # gives sum(ns) > n so let's force the group
    # specified in which.adjust to be a value that
    # makes it so that sum(ns) = n
    ns[which.adjust] <- n - sum(ns[-which.adjust])

    ids <- rep(1:length(props), ns)
    # Shuffle ids so that the groups are randomized
    which.group <- sample(ids)
    split(dat, which.group)
}

split = split_data(train, c(0.8, 0.2))
train1 = split$'1'
val1 = split$'2'

full.tree = rpart(SalePrice ~ . -Id,
                  data = train1,
                  method = "anova",
                  control = rpart.control(cp = 0.0, minsplit = 2))
plotcp(full.tree)

fit.tree = rpart(SalePrice ~ . -Id,
                 data = train1,
                 method = "anova",
                 control = rpart.control(cp = 0.0055, minsplit = 2))

sqrt(mean((val1$SalePrice - predict(fit.tree, val1, type = "vector"))^2))

#Try again with the full dataset and the resubstitution error
fit.tree1 = rpart(SalePrice ~ . -Id,
                  data = train,
                  method = "anova",
                  control = rpart.control(cp = 0.0055, minsplit = 2))

sqrt(mean((train$SalePrice - predict(fit.tree1, train, type = "vector"))^2))

```
Minimum xerror at cp
46  1.299978e-03     47 8.908836e-02 0.2499940 0.02580239
Using the 1 SE rule, we get
18  5.573976e-03     17 1.643091e-01 0.2697831 0.02541736

```{r randForests, cache = TRUE}
#Must first deal with NA values
deal_missing_values <- function(dataSet, ntrain = 1460, type = 1)
{
  # type is the replace method for numeric values. 
  #if type = 1 --> using mean to replace.
  #if type = 2 --> using median to replace.
  dataSet[] <- lapply(dataSet, function(x){
    # check if variables have a factor:
    if(!is.factor(x)) {
      #replace NA by mean
      if (type == 1) x[is.na(x)] <- mean(x[1:1460], na.rm = TRUE) 
      else if (type == 2) if (type == 1) x[is.na(x)] <- median(x[1:1460], na.rm = TRUE) 
    }
    else {
      # otherwise include NAs into factor levels and change factor levels:
      x <- factor(x, exclude=NULL)
      levels(x)[is.na(levels(x))] <- "Missing"
    }
    return(x)
  })
  
  return(dataSet)
}

train2 = deal_missing_values(train)

randF = randomForest(log(SalePrice) ~ . -Id,
                     data = train2,
                     importance = TRUE)
varImpPlot(randF)

```

```{r, include=FALSE}
encode_categorical_variables <- function(dataSet, ntrain=1460)
{
  dataSetTemp <- dataSet
  for (i in 1:ncol(dataSet)){
    if (is.factor(dataSet[,i])) {
      col_name = names(dataSet)[i]
      t <- model.matrix(~ factor(dataSet[,i]) - 1)
      dataSetTemp <- cbind(t[,2:ncol(t)], dataSetTemp)
      new_col_names <- paste0(col_name, "_bit", seq(1, ncol(t) - 1))
      names(dataSetTemp)[1:ncol(t) - 1] <- new_col_names
    }
  }
  removed <- c()
  for (i in 1:ncol(dataSetTemp)) {
    if (is.factor(dataSetTemp[,i])) removed <- c(removed, i)
  }
  dataSetTemp <- dataSetTemp[, -removed]
  return(dataSetTemp)
}

do_variable_selection <- function(dataSet)
{
  trainData <- dataSet[1:ntrain,]
  logSalePrice <- log(trainData$SalePrice)
  trainData <- cbind(trainData, logSalePrice)
  trainData$SalePrice <- NULL
  trainData.cv.lasso = cv.glmnet(x = as.matrix(trainData[,1:(ncol(trainData) - 1)]), 
                             y=as.matrix(trainData[,ncol(trainData)]), alpha = 1)
  names(trainData.cv.lasso$lambda.1se)="Lambda 1 SE:"
  trainData.cv.lasso$lambda.1se
  trainData.lasso = glmnet(x = as.matrix(trainData[,1:(ncol(trainData) - 1)]), 
                           y=as.matrix(trainData[,ncol(trainData)]), alpha = 1)
  co <- coef(trainData.lasso, s = trainData.cv.lasso$lambda.1se)
  co <- co[2:nrow(co),] #remove the coefficient of intercept, which is the first row
  trainData <- trainData[, co > 0]
  vars <- names(trainData)
  print('Important variables:')
  print(vars)
  vars <- c(vars, 'SalePrice')
  dataSet <- subset(dataSet, select = vars)
  return(dataSet)
}
```


```{r gbm, cache = TRUE}

```

```{r gbmTune, cache = TRUE}

```

```{r svm, cache = TRUE}

```

```{r svmTune, cache = TRUE}

```

```{r}
tenFoldCrossVal = function(formula, n = 10, data, type, nnn=1, ...){
  #n is the number of desired folds
  #data is the desired dataset
  #type is the type of analysis
  #formula is the formula for analysis
  require(dplyr)
  folds = rep(c(1:n), length = nrow(data))
  folds = sample(folds)
  
  resultVector = rep(0, length(nrow(data)))
  
  for (i in 1:n){
    train = data[folds!=i,]
    test = data[folds==i,]
    fit = switch(type,
                 rpart = rpart(formula, data = train, ...),
                 randomForest = randomForest(formula, data = train, ...),
                 lda = lda(formula, data = train, ...),
                 qda = qda(formula, data = train, ...),
                 knn3 = knn3(formula, data = train, ...),
                 glm = glm(formula, data = train, ...),
                 ada = ada(formula, data = train, ...),
                 svm = svm(formula, data = train, ...),
                 gbm = gbm(formula, data = train, ...),
                 neunet = neuralnet(formula,data=train,hidden=c(nnn)))
        
    resultVector[folds == i] = switch(type,
                                      rpart = predict(fit, newdata = test, ...),
                                      randomForest = predict(fit, newdata = test, ...),
                                      lda = predict(fit, data = test, ...)$class,
                                      qda = predict(fit, data = test, ...)$class,
                                      knn3 = predict(fit, data = test, ...),
                                      glm = predict(fit, data = test, ...),
                                      ada = predict(fit, data = test, ...),
                                      svm = predict(fit, data = test, ...),
                                      gbm = predict(fit, data = test, ...),
                                    neunet = compute(fit,covariate=select(test,-SalePrice))$net.result)
  }
  return(resultVector)
}
```


```{r neural net}


train_dum<-encode_categorical_variables(train2)
ntrain<-1460
train_dum_reduced<-do_variable_selection(train_dum)

vrs<-names(train_dum_reduced)

vrs[!vrs %in% "SalePrice"]
nn_form<-as.formula(paste("SalePrice ~", paste(vrs[!vrs %in% "SalePrice"],collapse= " + ")))

hpmeans<-apply(train_dum_reduced,2,mean)

train_dum_scaled<-as.data.frame(scale(train_dum_reduced, center=T,scale=T))

hnn1<-neuralnet(nn_form,data=train_dum_scaled,linear.output=T)

nn_cv_1node<-tenFoldCrossVal(formula=nn_form,data=train_dum_scaled,type="neunet",nnn=1)

nn_unscaled<-nn_cv_1node*sd(train_dum_reduced$SalePrice)+mean(train_dum_reduced$SalePrice)

RMSE<-sqrt(sum((train_dum_reduced$SalePrice - nn_unscaled)**2)/1460) 

#RMSE is extremely bad

test1<-deal_missing_values(test)
test_dum<-encode_categorical_variables(test1)
reduced_vars<-which(colnames(test_dum) %in% colnames(train_dum_reduced))

test_dum_reduced<-test_dum[,reduced_vars]

test_dum_scaled<-scale(test_dum_reduced)

prediction_nn<-compute(hnn1,covariate=test_dum_scaled)$net.result

prediction_nn<-prediction_nn*sd(train$SalePrice)+mean(train$SalePrice)
submission_nn<-data.frame(test$Id,prediction_nn)
colnames(submission_nn)<-c("ID","SalePrice")

write.csv(submission_nn,"submission_nn.csv",row.names = FALSE)

```

RMLSE for the 1 node, 1 layer neural net is .16521
